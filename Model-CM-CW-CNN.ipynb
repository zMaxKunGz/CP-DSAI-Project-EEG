{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Brain Inverders Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braininvaders2015a.dataset import BrainInvaders2015a\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BrainInvaders2015a()\n",
    "\n",
    "def loadData(subject, session = 'session_1', run = 'run_1'):\n",
    "    data = dataset._get_single_subject_data(subject)\n",
    "    data = data[session][run]\n",
    "    # data.set_montage(ten_twenty_montage)\n",
    "    return data\n",
    "\n",
    "data_subjects = []\n",
    "subjects = list(range(1,44))\n",
    "subjects.remove(1)\n",
    "subjects.remove(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    data_subjects.append(loadData(subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne import create_info\n",
    "from mne import Epochs, find_events\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def df_to_raw(df):\n",
    "    sfreq = 512\n",
    "    ch_names = list(df.columns)\n",
    "    ch_types = ['eeg'] * (len(df.columns) - 1) + ['stim']\n",
    "    ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "\n",
    "    df = df.T\n",
    "      #mne looks at the tranpose() format\n",
    "    df[:-1] *= 1e-6\n",
    "      #convert from uVolts to Volts (mne assumes Volts data)\n",
    "\n",
    "    info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "\n",
    "    raw = mne.io.RawArray(df, info)\n",
    "    raw.set_montage(ten_twenty_montage)\n",
    "    return raw\n",
    "\n",
    "def getEpochs(raw, event_id, tmin, tmax, picks):\n",
    "\n",
    "    #epoching\n",
    "    events = find_events(raw)\n",
    "    \n",
    "    #reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "    #                       grad=4000e-13,    # 4000 fT/cm\n",
    "    #                       eeg=100e-6,       # 150 μV\n",
    "    #                       eog=250e-6)       # 250 μV\n",
    "\n",
    "    reject_criteria = dict(eeg=100e-6)  #most voltage in this range is not brain components\n",
    "\n",
    "    epochs = Epochs(raw, events=events, event_id=event_id, \n",
    "                    tmin=tmin, tmax=tmax, baseline=None, preload=True,verbose=False, picks=picks)  #8 channels\n",
    "    print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "\n",
    "    return epochs\n",
    "  \n",
    "def preprocessing(rawdata, runPCA=False):\n",
    "    # Convert and drop time column\n",
    "    data_ses1_run1_pd = rawdata.to_data_frame()\n",
    "    data_ses1_run1_pd = data_ses1_run1_pd.drop(['time'],axis = 1)\n",
    "    raw = df_to_raw(data_ses1_run1_pd)\n",
    "\n",
    "    # Notch Filter\n",
    "    raw.notch_filter(np.arange(50, 251, 50))\n",
    "\n",
    "    eeg_channels = mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "    for cutoff in (0.1, 0.2, 1):\n",
    "      raw_highpass = raw.copy().filter(l_freq=cutoff, h_freq=None)\n",
    "      \n",
    "    raw.filter(1,24,method = 'iir')\n",
    "\n",
    "\n",
    "    if runPCA:\n",
    "      raw_df = raw.to_data_frame()\n",
    "      X1 = raw_df.drop(['time'],axis = 1)\n",
    "      X = X1.drop(['STI 014'],axis = 1)\n",
    "      y = raw_df['STI 014']\n",
    "      pca = PCA(n_components=32)\n",
    "      X = pca.fit(X.values).transform(X.values)\n",
    "      y1 = y.values.reshape(-1,1)\n",
    "      data = np.hstack((X,y1))\n",
    "      df = pd.DataFrame(data, columns = list(X1.columns))\n",
    "      raw = df_to_raw(df)\n",
    "\n",
    "    event_id = {'NonTarget': 1, 'Target': 2}\n",
    "    tmin = 0.0 #0\n",
    "    tmax = 1.0 #0.5 seconds\n",
    "    picks= eeg_channels\n",
    "    epochs = getEpochs(raw,event_id, tmin, tmax, picks)\n",
    "\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.events[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=33, n_times=129472\n",
      "    Range : 0 ... 129471 =      0.000 ...   252.873 secs\n",
      "Ready.\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 3381 samples (6.604 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 24 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 1.00, 24.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "360 events found\n",
      "Event IDs: [1 2]\n",
      "sample drop %:  0.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "X_subjects = []\n",
    "y_subjects = []\n",
    "runPCA = False\n",
    "\n",
    "for data in data_subjects:\n",
    "    X, y = preprocessing(data, runPCA=runPCA)    \n",
    "    X_subjects.append(X)\n",
    "    y_subjects.append(y)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reshape, Convert to torch, Test/Train Split, and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "def ShapePreparing(X, y, BATCH_SIZE = 32):\n",
    "    X_reshaped = X[:, np.newaxis, :, :]\n",
    "    torch_X_reshaped = torch.from_numpy(X_reshaped)\n",
    "    torch_y = torch.from_numpy(y)\n",
    "\n",
    "    ds = TensorDataset(torch_X_reshaped, torch_y)\n",
    "\n",
    "    #Train test split\n",
    "    train_size = int(round(torch_X_reshaped.size()[0] * 0.7))\n",
    "    valid_size = int(round(torch_X_reshaped.size()[0] * 0.1))\n",
    "    test_size = int(round(torch_X_reshaped.size()[0] * 0.2))\n",
    "    sum_size = np.sum([train_size, valid_size, test_size])\n",
    "\n",
    "    # Adjust total size to equal to sample size\n",
    "    while sum_size<torch_X_reshaped.shape[0]:\n",
    "        train_size += 1\n",
    "        sum_size = np.sum([train_size, valid_size, test_size])\n",
    "    while sum_size>torch_X_reshaped.shape[0]:\n",
    "        train_size -= 1\n",
    "        sum_size = np.sum([train_size, valid_size, test_size])\n",
    "    \n",
    "    # Split data\n",
    "    train_set, valid_set, test_set = torch.utils.data.random_split(ds, [train_size, valid_size, test_size])\n",
    "\n",
    "    #Train set loader\n",
    "    train_iterator = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=True)\n",
    "    #Validation set loader\n",
    "    valid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=True)\n",
    "\n",
    "    #Test set loader\n",
    "    test_iterator = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                            batch_size=test_size, \n",
    "                                            shuffle=True)\n",
    "    return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CM-CW-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_CW_CNN(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, height , width)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CM_CW_CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1,16,kernel_size=(32,1),stride=(1,1)))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16,16,kernel_size=(1,57),stride=(1,57)))\n",
    "        self.fc = nn.Sequential(nn.Linear(144,144),nn.ReLU(),nn.Dropout(0.5),\n",
    "                               nn.Linear(144,48),nn.ReLU(),nn.Dropout(0.5),\n",
    "                               nn.Linear(48,12),nn.ReLU(),nn.Dropout(0.5),\n",
    "                               nn.Linear(12,3),nn.ReLU(),nn.Dropout(0.1))\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # print(\"X\",x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"X\",x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(\"X\",x.shape)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        # print(\"X flatten\",x.shape)\n",
    "        # torch.manual_seed(9999)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, _print=False):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "\n",
    "    trues = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch, labels in iterator:\n",
    "        \n",
    "        #Move tensors to the configured device\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(batch.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        #check accuracy\n",
    "        predictions = model(batch.float())\n",
    "        _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "        if _print:\n",
    "            print('================== Predicted y ====================')\n",
    "            print(predicted)\n",
    "        predicteds.append(predicted)\n",
    "        total += labels.size(0)  #keep track of total\n",
    "        correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "        if _print:\n",
    "            print('==================    True y   ====================')\n",
    "            print(labels)\n",
    "        trues.append(labels)\n",
    "        acc = 100 * (correct / total)\n",
    "                \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc = acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc, predicteds, trues\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "    trues = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch, labels in iterator:\n",
    "            \n",
    "            #Move tensors to the configured device\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(batch.float())\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "            predicteds.append(predicted)\n",
    "            trues.append(labels)\n",
    "            total += labels.size(0)  #keep track of total\n",
    "            correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "            acc = 100 * (correct / total)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),predicteds, trues\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def saveModel(saveName, model, type_='Best'):\n",
    "    directory = f'results/{type(model).__name__}/{type_}/'\n",
    "    fileName = f'{saveName}.pth.tar'\n",
    "    path = directory+fileName\n",
    "    while True:\n",
    "        try:\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(\"Model:{} saved.\".format(fileName))\n",
    "            break\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "            open(path, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "\n",
    "def Training(model, train_iterator, valid_iterator, N_EPOCHS = 50, saveName=None):\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    start_time_train = time.time()\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        train_loss, train_acc, train_pred_label, train_true_label = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc, valid_pred_label, valid_true_label= evaluate(model, valid_iterator, criterion)\n",
    "        train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss); valid_accs.append(valid_acc)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Trainning:\", saveName)\n",
    "            print(f'Epoch: [{epoch+1:02}/{N_EPOCHS}]')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            if saveName != None:\n",
    "                saveModel(saveName, model, type_='Best')\n",
    "    if saveName != None:\n",
    "        saveModel(saveName, model, type_='Last')\n",
    "    training_time = time.time() - start_time_train\n",
    "    return train_losses, valid_losses, train_accs, valid_accs, training_time, best_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning: woPCA_43\n",
      "Epoch: [100/100]\n",
      "\tTrain Loss: 0.180 | Train Acc: 86.90%\n",
      "\t Val. Loss: 0.269 |  Val. Acc: 94.44%\n",
      "Model:woPCA_43.pth.tar saved.\n"
     ]
    }
   ],
   "source": [
    "train_losses_list = []\n",
    "train_accs_list = []\n",
    "valid_losses_list = []\n",
    "valid_accs_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "training_time_list = []\n",
    "best_epoch_list = []\n",
    "test_iterator_list = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Configured device: \", device)\n",
    "\n",
    "# for i in range(2):\n",
    "for i in range(len(X_subjects)):\n",
    "    # Split data\n",
    "    train_iterator, valid_iterator, test_iterator = ShapePreparing(X_subjects[i], y_subjects[i], BATCH_SIZE=64)\n",
    "    \n",
    "    # Define model\n",
    "    model = CM_CW_CNN()\n",
    "    model = model.float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    # Training\n",
    "    fname = \"PCA\" if runPCA else \"woPCA\"\n",
    "    if subjects[i]<10:\n",
    "        filename = f'{fname}_0{str(subjects[i])}'\n",
    "    else:\n",
    "        filename = f'{fname}_{str(subjects[i])}'\n",
    "    # filename = None\n",
    "    print(filename)\n",
    "    train_losses, valid_losses, train_accs, valid_accs, training_time, best_epoch = Training(model, train_iterator, valid_iterator, N_EPOCHS = 100, saveName=filename)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator, criterion)\n",
    "    # Record results\n",
    "    train_losses_list.append(train_losses); train_accs_list.append(train_accs)\n",
    "    valid_losses_list.append(valid_losses); valid_accs_list.append(valid_accs)\n",
    "    test_loss_list.append(test_loss); test_acc_list.append(test_acc)\n",
    "    training_time_list.append(training_time); best_epoch_list.append(best_epoch)\n",
    "    test_iterator_list.append(test_iterator)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"train_losses\", \"valid_losses\", \"test_loss\", \n",
    "    \"train_accs\", \"test_loss\", \"test_acc\", \n",
    "    \"training_time\", \"best_epoch\"]\n",
    "best_model_result = pd.DataFrame(columns=col, index=subjects)\n",
    "last_model_result = pd.DataFrame(columns=col, index=subjects)\n",
    "\n",
    "best_model_result.index.name = \"subjects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_losses</th>\n",
       "      <th>valid_losses</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_accs</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>training_time</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjects</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162368</td>\n",
       "      <td>0.467681</td>\n",
       "      <td>0.424221</td>\n",
       "      <td>88.073394</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>88.297872</td>\n",
       "      <td>4.013003</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.152798</td>\n",
       "      <td>0.606242</td>\n",
       "      <td>0.211724</td>\n",
       "      <td>87.698413</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>2.941998</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.208705</td>\n",
       "      <td>0.441094</td>\n",
       "      <td>0.533676</td>\n",
       "      <td>84.158416</td>\n",
       "      <td>93.023256</td>\n",
       "      <td>89.534884</td>\n",
       "      <td>3.590024</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.226736</td>\n",
       "      <td>0.777465</td>\n",
       "      <td>0.542523</td>\n",
       "      <td>87.458746</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>3.477011</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.239926</td>\n",
       "      <td>1.558826</td>\n",
       "      <td>1.011654</td>\n",
       "      <td>81.746032</td>\n",
       "      <td>85.185185</td>\n",
       "      <td>86.111111</td>\n",
       "      <td>4.348999</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.256297</td>\n",
       "      <td>0.278458</td>\n",
       "      <td>0.453765</td>\n",
       "      <td>80.555556</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>87.5</td>\n",
       "      <td>3.122995</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.210315</td>\n",
       "      <td>1.040325</td>\n",
       "      <td>0.714219</td>\n",
       "      <td>87.788779</td>\n",
       "      <td>88.372093</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>3.498001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.156962</td>\n",
       "      <td>0.815418</td>\n",
       "      <td>0.394539</td>\n",
       "      <td>90.21164</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>93.518519</td>\n",
       "      <td>4.172001</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.213591</td>\n",
       "      <td>0.582537</td>\n",
       "      <td>0.471594</td>\n",
       "      <td>83.700441</td>\n",
       "      <td>90.625</td>\n",
       "      <td>89.230769</td>\n",
       "      <td>2.726</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.322589</td>\n",
       "      <td>0.526758</td>\n",
       "      <td>0.769672</td>\n",
       "      <td>74.00722</td>\n",
       "      <td>80.0</td>\n",
       "      <td>87.341772</td>\n",
       "      <td>3.482</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.192535</td>\n",
       "      <td>0.966488</td>\n",
       "      <td>1.178019</td>\n",
       "      <td>88.091603</td>\n",
       "      <td>86.303191</td>\n",
       "      <td>87.700535</td>\n",
       "      <td>6.823005</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.239278</td>\n",
       "      <td>1.330271</td>\n",
       "      <td>0.926581</td>\n",
       "      <td>80.15873</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>3.694018</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.29197</td>\n",
       "      <td>1.764398</td>\n",
       "      <td>0.704319</td>\n",
       "      <td>79.156328</td>\n",
       "      <td>77.586207</td>\n",
       "      <td>83.478261</td>\n",
       "      <td>4.094018</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.245369</td>\n",
       "      <td>1.017307</td>\n",
       "      <td>0.15942</td>\n",
       "      <td>78.968254</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>2.757995</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.21551</td>\n",
       "      <td>0.383009</td>\n",
       "      <td>0.699371</td>\n",
       "      <td>84.837545</td>\n",
       "      <td>92.5</td>\n",
       "      <td>89.873418</td>\n",
       "      <td>3.026997</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.173946</td>\n",
       "      <td>0.460636</td>\n",
       "      <td>1.345222</td>\n",
       "      <td>89.74359</td>\n",
       "      <td>90.163934</td>\n",
       "      <td>86.065574</td>\n",
       "      <td>4.575001</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.239774</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>2.40934</td>\n",
       "      <td>88.778878</td>\n",
       "      <td>88.372093</td>\n",
       "      <td>79.069767</td>\n",
       "      <td>3.246999</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.148732</td>\n",
       "      <td>2.05518</td>\n",
       "      <td>1.14228</td>\n",
       "      <td>86.507937</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>86.111111</td>\n",
       "      <td>3.718</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.232087</td>\n",
       "      <td>1.028167</td>\n",
       "      <td>1.166641</td>\n",
       "      <td>83.49835</td>\n",
       "      <td>88.372093</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>3.123001</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.257151</td>\n",
       "      <td>0.428583</td>\n",
       "      <td>0.717056</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>2.568002</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.221536</td>\n",
       "      <td>1.184311</td>\n",
       "      <td>1.552369</td>\n",
       "      <td>87.535411</td>\n",
       "      <td>82.0</td>\n",
       "      <td>84.158416</td>\n",
       "      <td>3.624</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.221165</td>\n",
       "      <td>0.980093</td>\n",
       "      <td>0.767043</td>\n",
       "      <td>90.084986</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.118812</td>\n",
       "      <td>3.543</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.280147</td>\n",
       "      <td>0.475344</td>\n",
       "      <td>1.067322</td>\n",
       "      <td>79.422383</td>\n",
       "      <td>82.5</td>\n",
       "      <td>79.746835</td>\n",
       "      <td>3.021001</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.24918</td>\n",
       "      <td>0.229485</td>\n",
       "      <td>0.602827</td>\n",
       "      <td>81.389578</td>\n",
       "      <td>96.551724</td>\n",
       "      <td>87.826087</td>\n",
       "      <td>4.110004</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.322871</td>\n",
       "      <td>0.4346</td>\n",
       "      <td>0.369315</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>80.555556</td>\n",
       "      <td>2.802002</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.206208</td>\n",
       "      <td>1.744696</td>\n",
       "      <td>1.195182</td>\n",
       "      <td>89.352818</td>\n",
       "      <td>83.363971</td>\n",
       "      <td>81.751825</td>\n",
       "      <td>4.725005</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.213713</td>\n",
       "      <td>0.736384</td>\n",
       "      <td>2.28927</td>\n",
       "      <td>88.08933</td>\n",
       "      <td>93.103448</td>\n",
       "      <td>84.347826</td>\n",
       "      <td>4.046001</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.640529</td>\n",
       "      <td>0.404773</td>\n",
       "      <td>0.41047</td>\n",
       "      <td>73.127753</td>\n",
       "      <td>87.5</td>\n",
       "      <td>84.615385</td>\n",
       "      <td>2.595999</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.151801</td>\n",
       "      <td>1.026174</td>\n",
       "      <td>1.826838</td>\n",
       "      <td>88.412698</td>\n",
       "      <td>88.194444</td>\n",
       "      <td>82.777778</td>\n",
       "      <td>5.952</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.205688</td>\n",
       "      <td>2.907257</td>\n",
       "      <td>0.927666</td>\n",
       "      <td>87.301587</td>\n",
       "      <td>80.555556</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>2.528</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.259021</td>\n",
       "      <td>1.26675</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>84.920635</td>\n",
       "      <td>79.62963</td>\n",
       "      <td>86.111111</td>\n",
       "      <td>3.711</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.26545</td>\n",
       "      <td>0.359614</td>\n",
       "      <td>0.707802</td>\n",
       "      <td>89.296636</td>\n",
       "      <td>87.234043</td>\n",
       "      <td>86.170213</td>\n",
       "      <td>3.705</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.202438</td>\n",
       "      <td>1.36267</td>\n",
       "      <td>1.673711</td>\n",
       "      <td>87.22467</td>\n",
       "      <td>82.643363</td>\n",
       "      <td>78.350515</td>\n",
       "      <td>6.46901</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.172037</td>\n",
       "      <td>0.663112</td>\n",
       "      <td>0.563514</td>\n",
       "      <td>86.642599</td>\n",
       "      <td>80.0</td>\n",
       "      <td>92.405063</td>\n",
       "      <td>3.165</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.364223</td>\n",
       "      <td>0.331311</td>\n",
       "      <td>0.434633</td>\n",
       "      <td>70.642202</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>84.042553</td>\n",
       "      <td>3.635</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.315928</td>\n",
       "      <td>0.592453</td>\n",
       "      <td>0.93378</td>\n",
       "      <td>75.535168</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>77.659574</td>\n",
       "      <td>3.593003</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.193205</td>\n",
       "      <td>1.263121</td>\n",
       "      <td>0.960427</td>\n",
       "      <td>83.870968</td>\n",
       "      <td>82.758621</td>\n",
       "      <td>86.086957</td>\n",
       "      <td>3.992001</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.178568</td>\n",
       "      <td>1.011553</td>\n",
       "      <td>1.357045</td>\n",
       "      <td>90.20979</td>\n",
       "      <td>86.885246</td>\n",
       "      <td>87.704918</td>\n",
       "      <td>4.185999</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.40005</td>\n",
       "      <td>0.454138</td>\n",
       "      <td>1.452431</td>\n",
       "      <td>74.787535</td>\n",
       "      <td>96.0</td>\n",
       "      <td>86.138614</td>\n",
       "      <td>3.789002</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.330761</td>\n",
       "      <td>0.353212</td>\n",
       "      <td>1.602934</td>\n",
       "      <td>88.08933</td>\n",
       "      <td>89.655172</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4.104</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.293336</td>\n",
       "      <td>0.587176</td>\n",
       "      <td>0.251267</td>\n",
       "      <td>76.984127</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>94.444444</td>\n",
       "      <td>2.59</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_losses valid_losses test_loss train_accs  test_loss   test_acc  \\\n",
       "subjects                                                                        \n",
       "2            0.162368     0.467681  0.424221  88.073394  85.106383  88.297872   \n",
       "3            0.152798     0.606242  0.211724  87.698413  94.444444  91.666667   \n",
       "4            0.208705     0.441094  0.533676  84.158416  93.023256  89.534884   \n",
       "5            0.226736     0.777465  0.542523  87.458746  86.046512  90.697674   \n",
       "6            0.239926     1.558826  1.011654  81.746032  85.185185  86.111111   \n",
       "7            0.256297     0.278458  0.453765  80.555556  94.444444       87.5   \n",
       "8            0.210315     1.040325  0.714219  87.788779  88.372093  86.046512   \n",
       "9            0.156962     0.815418  0.394539   90.21164  87.037037  93.518519   \n",
       "10           0.213591     0.582537  0.471594  83.700441     90.625  89.230769   \n",
       "11           0.322589     0.526758  0.769672   74.00722       80.0  87.341772   \n",
       "12           0.192535     0.966488  1.178019  88.091603  86.303191  87.700535   \n",
       "13           0.239278     1.330271  0.926581   80.15873  87.037037  88.888889   \n",
       "14            0.29197     1.764398  0.704319  79.156328  77.586207  83.478261   \n",
       "15           0.245369     1.017307   0.15942  78.968254  88.888889  94.444444   \n",
       "16            0.21551     0.383009  0.699371  84.837545       92.5  89.873418   \n",
       "17           0.173946     0.460636  1.345222   89.74359  90.163934  86.065574   \n",
       "18           0.239774     0.967708   2.40934  88.778878  88.372093  79.069767   \n",
       "19           0.148732      2.05518   1.14228  86.507937  81.481481  86.111111   \n",
       "20           0.232087     1.028167  1.166641   83.49835  88.372093  90.697674   \n",
       "21           0.257151     0.428583  0.717056  83.333333  83.333333  83.333333   \n",
       "22           0.221536     1.184311  1.552369  87.535411       82.0  84.158416   \n",
       "23           0.221165     0.980093  0.767043  90.084986       90.0  88.118812   \n",
       "24           0.280147     0.475344  1.067322  79.422383       82.5  79.746835   \n",
       "25            0.24918     0.229485  0.602827  81.389578  96.551724  87.826087   \n",
       "26           0.322871       0.4346  0.369315  71.428571  91.666667  80.555556   \n",
       "28           0.206208     1.744696  1.195182  89.352818  83.363971  81.751825   \n",
       "29           0.213713     0.736384   2.28927   88.08933  93.103448  84.347826   \n",
       "30           0.640529     0.404773   0.41047  73.127753       87.5  84.615385   \n",
       "31           0.151801     1.026174  1.826838  88.412698  88.194444  82.777778   \n",
       "32           0.205688     2.907257  0.927666  87.301587  80.555556  91.666667   \n",
       "33           0.259021      1.26675  0.987302  84.920635   79.62963  86.111111   \n",
       "34            0.26545     0.359614  0.707802  89.296636  87.234043  86.170213   \n",
       "35           0.202438      1.36267  1.673711   87.22467  82.643363  78.350515   \n",
       "36           0.172037     0.663112  0.563514  86.642599       80.0  92.405063   \n",
       "37           0.364223     0.331311  0.434633  70.642202  85.106383  84.042553   \n",
       "38           0.315928     0.592453   0.93378  75.535168  85.106383  77.659574   \n",
       "39           0.193205     1.263121  0.960427  83.870968  82.758621  86.086957   \n",
       "40           0.178568     1.011553  1.357045   90.20979  86.885246  87.704918   \n",
       "41            0.40005     0.454138  1.452431  74.787535       96.0  86.138614   \n",
       "42           0.330761     0.353212  1.602934   88.08933  89.655172       80.0   \n",
       "43           0.293336     0.587176  0.251267  76.984127  88.888889  94.444444   \n",
       "\n",
       "         training_time best_epoch  \n",
       "subjects                           \n",
       "2             4.013003         44  \n",
       "3             2.941998         44  \n",
       "4             3.590024         22  \n",
       "5             3.477011         18  \n",
       "6             4.348999         13  \n",
       "7             3.122995         42  \n",
       "8             3.498001          9  \n",
       "9             4.172001         43  \n",
       "10               2.726         18  \n",
       "11               3.482         27  \n",
       "12            6.823005         18  \n",
       "13            3.694018         12  \n",
       "14            4.094018          6  \n",
       "15            2.757995         17  \n",
       "16            3.026997         27  \n",
       "17            4.575001         37  \n",
       "18            3.246999         18  \n",
       "19               3.718         10  \n",
       "20            3.123001         18  \n",
       "21            2.568002         37  \n",
       "22               3.624         17  \n",
       "23               3.543          7  \n",
       "24            3.021001         36  \n",
       "25            4.110004         49  \n",
       "26            2.802002         31  \n",
       "28            4.725005         26  \n",
       "29            4.046001         18  \n",
       "30            2.595999         49  \n",
       "31               5.952         13  \n",
       "32               2.528         14  \n",
       "33               3.711         17  \n",
       "34               3.705         43  \n",
       "35             6.46901         11  \n",
       "36               3.165         26  \n",
       "37               3.635         34  \n",
       "38            3.593003         24  \n",
       "39            3.992001         12  \n",
       "40            4.185999         10  \n",
       "41            3.789002         43  \n",
       "42               4.104         23  \n",
       "43                2.59         38  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [\"train_losses\", \"valid_losses\", \"test_loss\", \n",
    "    \"train_accs\", \"test_loss\", \"test_acc\", \n",
    "    \"training_time\", \"best_epoch\"]\n",
    "df = pd.DataFrame(columns=col, index=subjects)\n",
    "df.index.name = \"subjects\"\n",
    "\n",
    "for i in range(len(train_losses_list)):\n",
    "    df.loc[subjects[i]] = [train_losses_list[i][-1], valid_losses_list[i][-1], test_loss_list[i], \n",
    "        train_accs_list[i][-1], valid_accs_list[i][-1], test_acc_list[i],\n",
    "        training_time_list[i], best_epoch_list[i]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results\\CM_CW_CNN\\CM_CW_CNN_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_losses      0.245719\n",
       "valid_losses      0.874751\n",
       "test_loss         0.924609\n",
       "train_accs       83.727365\n",
       "test_loss        87.016248\n",
       "test_acc         86.446047\n",
       "training_time     3.728905\n",
       "best_epoch       24.902439\n",
       "dtype: float64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Best model ====\n",
      "max:  94.44444444444444\n",
      "min:  74.46808510638297\n",
      "mean:  84.3597388036286\n",
      "==== Last (Load) model ====\n",
      "max:  94.44444444444444\n",
      "min:  77.6595744680851\n",
      "mean:  86.44604720342089\n",
      "==== Last model ====\n",
      "max:  94.44444444444444\n",
      "min:  77.6595744680851\n",
      "mean:  86.44604720342089\n"
     ]
    }
   ],
   "source": [
    "def trainedModelLoader(path, device):\n",
    "    model = CM_CW_CNN()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    return model, criterion\n",
    "\n",
    "test_loss_best = []\n",
    "test_acc_best = []\n",
    "test_loss_last = []\n",
    "test_acc_last = []\n",
    "\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    fname = \"PCA\" if runPCA else fname = \"woPCA\"\n",
    "    if subject<10:\n",
    "        filename = f'{fname}_0{str(subject)}'\n",
    "    else:\n",
    "        filename = f'{fname}_{str(subject)}'\n",
    "    # Best model\n",
    "    type_ = 'Best'\n",
    "    path = f'results/CM_CW_CNN/{type_}/{filename}.pth.tar'\n",
    "    model, criterion = trainedModelLoader(path, device)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator_list[i], criterion)\n",
    "    test_loss_best.append(test_loss)\n",
    "    test_acc_best.append(test_acc)\n",
    "    # Last model\n",
    "    type_ = 'Last'\n",
    "    path = f'results/CM_CW_CNN/{type_}/{filename}.pth.tar'\n",
    "    model, criterion = trainedModelLoader(path, device)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator_list[i], criterion)\n",
    "    test_loss_last.append(test_loss)\n",
    "    test_acc_last.append(test_acc)\n",
    "\n",
    "\n",
    "print(\"==== Best model ====\")\n",
    "# print(test_acc_best)\n",
    "print(\"max: \", np.max(test_acc_best))\n",
    "print(\"min: \", np.min(test_acc_best))\n",
    "print(\"mean: \", np.mean(test_acc_best))\n",
    "\n",
    "print(\"==== Last (Load) model ====\")\n",
    "# print(test_acc_list)\n",
    "print(\"max: \", np.max(test_acc_last))\n",
    "print(\"min: \", np.min(test_acc_last))\n",
    "print(\"mean: \", np.mean(test_acc_last))\n",
    "\n",
    "print(\"==== Last model ====\")\n",
    "# print(test_acc_list)\n",
    "print(\"max: \", np.max(test_acc_list))\n",
    "print(\"min: \", np.min(test_acc_list))\n",
    "print(\"mean: \", np.mean(test_acc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda.is_available():\n",
    "#     net = model.cuda()\n",
    "#     X_test_tensor = X_test_tensor.cuda()\n",
    "# else:\n",
    "#     net = net.cpu()\n",
    "#     X_test_tensor = X_test_tensor.cpu()\n",
    "\n",
    "# output = net(X_test_tensor)\n",
    "# _, predicted = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(train_losses, label=\"train\")\n",
    "# plt.plot(valid_losses, label=\"validation\")\n",
    "# plt.title(\"Losses\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_accs, label=\"train\")\n",
    "# plt.plot(valid_accs, label=\"validation\")\n",
    "# plt.title(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
