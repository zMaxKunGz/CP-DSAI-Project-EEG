{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Brain Inverders Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braininvaders2015a.dataset import BrainInvaders2015a\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BrainInvaders2015a()\n",
    "\n",
    "def loadData(subject, session = 'session_1', run = 'run_1'):\n",
    "    data = dataset._get_single_subject_data(subject)\n",
    "    data = data[session][run]\n",
    "    # data.set_montage(ten_twenty_montage)\n",
    "    return data\n",
    "\n",
    "data_subjects = []\n",
    "subjects = list(range(1,44))\n",
    "subjects.remove(1)\n",
    "subjects.remove(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    data_subjects.append(loadData(subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from mne import create_info\n",
    "from mne import Epochs, find_events\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def df_to_raw(df):\n",
    "    sfreq = 512\n",
    "    ch_names = list(df.columns)\n",
    "    ch_types = ['eeg'] * (len(df.columns) - 1) + ['stim']\n",
    "    ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\n",
    "\n",
    "    df = df.T\n",
    "      #mne looks at the tranpose() format\n",
    "    df[:-1] *= 1e-6\n",
    "      #convert from uVolts to Volts (mne assumes Volts data)\n",
    "\n",
    "    info = create_info(ch_names=ch_names, ch_types=ch_types, sfreq=sfreq)\n",
    "\n",
    "    raw = mne.io.RawArray(df, info)\n",
    "    raw.set_montage(ten_twenty_montage)\n",
    "    return raw\n",
    "\n",
    "def getEpochs(raw, event_id, tmin, tmax, picks):\n",
    "\n",
    "    #epoching\n",
    "    events = find_events(raw)\n",
    "    \n",
    "    #reject_criteria = dict(mag=4000e-15,     # 4000 fT\n",
    "    #                       grad=4000e-13,    # 4000 fT/cm\n",
    "    #                       eeg=100e-6,       # 150 μV\n",
    "    #                       eog=250e-6)       # 250 μV\n",
    "\n",
    "    reject_criteria = dict(eeg=100e-6)  #most voltage in this range is not brain components\n",
    "\n",
    "    epochs = Epochs(raw, events=events, event_id=event_id, \n",
    "                    tmin=tmin, tmax=tmax, baseline=None, preload=True,verbose=False, picks=picks)  #8 channels\n",
    "    print('sample drop %: ', (1 - len(epochs.events)/len(events)) * 100)\n",
    "\n",
    "    return epochs\n",
    "  \n",
    "def preprocessing(rawdata, runPCA=False):\n",
    "    # Convert and drop time column\n",
    "    data_ses1_run1_pd = rawdata.to_data_frame()\n",
    "    data_ses1_run1_pd = data_ses1_run1_pd.drop(['time'],axis = 1)\n",
    "    raw = df_to_raw(data_ses1_run1_pd)\n",
    "\n",
    "    # Notch Filter\n",
    "    raw.notch_filter(np.arange(50, 251, 50))\n",
    "\n",
    "    eeg_channels = mne.pick_types(raw.info, eeg=True)\n",
    "      \n",
    "    raw.filter(1,24,method = 'iir')\n",
    "\n",
    "\n",
    "    if runPCA:\n",
    "      raw_df = raw.to_data_frame()\n",
    "      X1 = raw_df.drop(['time'],axis = 1)\n",
    "      X = X1.drop(['STI 014'],axis = 1)\n",
    "      y = raw_df['STI 014']\n",
    "      pca = PCA(n_components=32)\n",
    "      X = pca.fit(X.values).transform(X.values)\n",
    "      y1 = y.values.reshape(-1,1)\n",
    "      data = np.hstack((X,y1))\n",
    "      df = pd.DataFrame(data, columns = list(X1.columns))\n",
    "      raw = df_to_raw(df)\n",
    "\n",
    "    event_id = {'NonTarget': 1, 'Target': 2}\n",
    "    tmin = 0.0 #0\n",
    "    tmax = 1.0 #0.5 seconds\n",
    "    picks= eeg_channels\n",
    "    epochs = getEpochs(raw,event_id, tmin, tmax, picks)\n",
    "\n",
    "    X = epochs.get_data()\n",
    "    y = epochs.events[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=33, n_times=129472\n",
      "    Range : 0 ... 129471 =      0.000 ...   252.873 secs\n",
      "Ready.\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 3381 samples (6.604 sec)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 24 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 1.00, 24.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "360 events found\n",
      "Event IDs: [1 2]\n",
      "sample drop %:  0.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "X_subjects = []\n",
    "y_subjects = []\n",
    "runPCA = False\n",
    "\n",
    "for data in data_subjects:\n",
    "    X, y = preprocessing(data, runPCA=runPCA)    \n",
    "    X_subjects.append(X)\n",
    "    y_subjects.append(y)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reshape, Convert to torch, Test/Train Split, and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "def ShapePreparing(X, y, BATCH_SIZE = 32):\n",
    "    X_reshaped = X[:, np.newaxis, :, :]\n",
    "    torch_X_reshaped = torch.from_numpy(X_reshaped)\n",
    "    torch_y = torch.from_numpy(y)\n",
    "\n",
    "    ds = TensorDataset(torch_X_reshaped, torch_y)\n",
    "\n",
    "    #Train test split\n",
    "    train_size = int(round(torch_X_reshaped.size()[0] * 0.7))\n",
    "    valid_size = int(round(torch_X_reshaped.size()[0] * 0.1))\n",
    "    test_size = int(round(torch_X_reshaped.size()[0] * 0.2))\n",
    "    sum_size = np.sum([train_size, valid_size, test_size])\n",
    "\n",
    "    # Adjust total size to equal to sample size\n",
    "    while sum_size<torch_X_reshaped.shape[0]:\n",
    "        train_size += 1\n",
    "        sum_size = np.sum([train_size, valid_size, test_size])\n",
    "    while sum_size>torch_X_reshaped.shape[0]:\n",
    "        train_size -= 1\n",
    "        sum_size = np.sum([train_size, valid_size, test_size])\n",
    "    \n",
    "    # Split data\n",
    "    train_set, valid_set, test_set = torch.utils.data.random_split(ds, [train_size, valid_size, test_size])\n",
    "\n",
    "    #Train set loader\n",
    "    train_iterator = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=True)\n",
    "    #Validation set loader\n",
    "    valid_iterator = torch.utils.data.DataLoader(dataset=valid_set, \n",
    "                                            batch_size=BATCH_SIZE, \n",
    "                                            shuffle=True)\n",
    "\n",
    "    #Test set loader\n",
    "    test_iterator = torch.utils.data.DataLoader(dataset=test_set, \n",
    "                                            batch_size=test_size, \n",
    "                                            shuffle=True)\n",
    "    return train_iterator, valid_iterator, test_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CM-CW-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CM_CW_CNN(nn.Module):\n",
    "    '''\n",
    "    Expected Input Shape: (batch, channels, height , width)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(CM_CW_CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1,16,kernel_size=(32,1),stride=(1,1)))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16,16,kernel_size=(1,57),stride=(1,57)))\n",
    "        self.fc = nn.Sequential(nn.Linear(144,144),nn.ReLU(),nn.Dropout(0.5),\n",
    "                               nn.Linear(144,48),nn.ReLU(),nn.Dropout(0.75),\n",
    "                               nn.Linear(48,12),nn.ReLU(),nn.Dropout(0.75),\n",
    "                               nn.Linear(12,3),nn.ReLU(),nn.Dropout(0.1))\n",
    "        # self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # print(\"X\",x.shape)\n",
    "        x = self.conv1(x)\n",
    "        # print(\"X\",x.shape)\n",
    "        x = self.conv2(x)\n",
    "        # print(\"X\",x.shape)\n",
    "        x = x.flatten(start_dim = 1)\n",
    "        # print(\"X flatten\",x.shape)\n",
    "        # torch.manual_seed(9999)\n",
    "        x = self.fc(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, _print=False):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "\n",
    "    trues = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch, labels in iterator:\n",
    "        \n",
    "        #Move tensors to the configured device\n",
    "        batch = batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(batch.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        #check accuracy\n",
    "        predictions = model(batch.float())\n",
    "        _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "        if _print:\n",
    "            print('================== Predicted y ====================')\n",
    "            print(predicted)\n",
    "        predicteds.append(predicted)\n",
    "        total += labels.size(0)  #keep track of total\n",
    "        correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "        if _print:\n",
    "            print('==================    True y   ====================')\n",
    "            print(labels)\n",
    "        trues.append(labels)\n",
    "        acc = 100 * (correct / total)\n",
    "                \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc = acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc, predicteds, trues\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    predicteds = []\n",
    "    trues = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch, labels in iterator:\n",
    "            \n",
    "            #Move tensors to the configured device\n",
    "            batch = batch.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(batch.float())\n",
    "            loss = criterion(predictions, labels)\n",
    "\n",
    "            _, predicted = torch.max(predictions.data, 1)  #returns max value, indices\n",
    "            predicteds.append(predicted)\n",
    "            trues.append(labels)\n",
    "            total += labels.size(0)  #keep track of total\n",
    "            correct += (predicted == labels).sum().item()  #.item() give the raw number\n",
    "            acc = 100 * (correct / total)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator),predicteds, trues\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def saveModel(saveName, model, type_='Best'):\n",
    "    directory = f'results/{type(model).__name__}/{type_}/'\n",
    "    fileName = f'{saveName}.pth.tar'\n",
    "    path = directory+fileName\n",
    "    while True:\n",
    "        try:\n",
    "            torch.save(model.state_dict(), path)\n",
    "            print(\"Model:{} saved.\".format(fileName))\n",
    "            break\n",
    "        except:\n",
    "            os.mkdir(directory)\n",
    "            open(path, 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import os\n",
    "\n",
    "\n",
    "def Training(model, train_iterator, valid_iterator, N_EPOCHS = 50, saveName=None):\n",
    "    \n",
    "    best_valid_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "    \n",
    "    start_time_train = time.time()\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        train_loss, train_acc, train_pred_label, train_true_label = train(model, train_iterator, optimizer, criterion)\n",
    "        valid_loss, valid_acc, valid_pred_label, valid_true_label= evaluate(model, valid_iterator, criterion)\n",
    "        train_losses.append(train_loss); train_accs.append(train_acc)\n",
    "        valid_losses.append(valid_loss); valid_accs.append(valid_acc)\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Trainning:\", saveName)\n",
    "            print(f'Epoch: [{epoch+1:02}/{N_EPOCHS}]')\n",
    "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            if saveName != None:\n",
    "                saveModel(saveName, model, type_='Best')\n",
    "    if saveName != None:\n",
    "        saveModel(saveName, model, type_='Last')\n",
    "    training_time = time.time() - start_time_train\n",
    "    return train_losses, valid_losses, train_accs, valid_accs, training_time, best_epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainning: woPCA_43\n",
      "Epoch: [100/100]\n",
      "\tTrain Loss: 0.415 | Train Acc: 78.17%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 83.33%\n",
      "Model:woPCA_43.pth.tar saved.\n"
     ]
    }
   ],
   "source": [
    "train_losses_list = []\n",
    "train_accs_list = []\n",
    "valid_losses_list = []\n",
    "valid_accs_list = []\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "training_time_list = []\n",
    "best_epoch_list = []\n",
    "test_iterator_list = []\n",
    "\n",
    "learning_rate = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Configured device: \", device)\n",
    "\n",
    "# for i in range(2):\n",
    "for i in range(len(X_subjects)):\n",
    "    # Split data\n",
    "    train_iterator, valid_iterator, test_iterator = ShapePreparing(X_subjects[i], y_subjects[i], BATCH_SIZE=64)\n",
    "    \n",
    "    # Define model\n",
    "    model = CM_CW_CNN()\n",
    "    model = model.float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    \n",
    "    # Training\n",
    "    fname = \"PCA\" if runPCA else \"woPCA\"\n",
    "    if subjects[i]<10:\n",
    "        filename = f'{fname}_0{str(subjects[i])}'\n",
    "    else:\n",
    "        filename = f'{fname}_{str(subjects[i])}'\n",
    "    # filename = None\n",
    "    print(filename)\n",
    "    train_losses, valid_losses, train_accs, valid_accs, training_time, best_epoch = Training(model, train_iterator, valid_iterator, N_EPOCHS = 100, saveName=filename)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator, criterion)\n",
    "    # Record results\n",
    "    train_losses_list.append(train_losses); train_accs_list.append(train_accs)\n",
    "    valid_losses_list.append(valid_losses); valid_accs_list.append(valid_accs)\n",
    "    test_loss_list.append(test_loss); test_acc_list.append(test_acc)\n",
    "    training_time_list.append(training_time); best_epoch_list.append(best_epoch)\n",
    "    test_iterator_list.append(test_iterator)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\"train_losses\", \"valid_losses\", \"test_loss\", \n",
    "    \"train_accs\", \"test_loss\", \"test_acc\", \n",
    "    \"training_time\", \"best_epoch\"]\n",
    "best_model_result = pd.DataFrame(columns=col, index=subjects)\n",
    "last_model_result = pd.DataFrame(columns=col, index=subjects)\n",
    "\n",
    "best_model_result.index.name = \"subjects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_losses</th>\n",
       "      <th>valid_losses</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>train_accs</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>training_time</th>\n",
       "      <th>best_epoch</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjects</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.421393</td>\n",
       "      <td>1.349322</td>\n",
       "      <td>1.5347</td>\n",
       "      <td>85.626911</td>\n",
       "      <td>80.851064</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>6.29099</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.401305</td>\n",
       "      <td>0.297251</td>\n",
       "      <td>0.195776</td>\n",
       "      <td>78.174603</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>93.055556</td>\n",
       "      <td>4.584998</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.370118</td>\n",
       "      <td>0.713013</td>\n",
       "      <td>0.899133</td>\n",
       "      <td>75.577558</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>5.083001</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.357124</td>\n",
       "      <td>0.704381</td>\n",
       "      <td>0.892578</td>\n",
       "      <td>84.158416</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>82.55814</td>\n",
       "      <td>5.149</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.303355</td>\n",
       "      <td>0.951757</td>\n",
       "      <td>1.528855</td>\n",
       "      <td>79.89418</td>\n",
       "      <td>85.185185</td>\n",
       "      <td>86.111111</td>\n",
       "      <td>6.293001</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.37424</td>\n",
       "      <td>1.160006</td>\n",
       "      <td>0.525571</td>\n",
       "      <td>79.761905</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>87.5</td>\n",
       "      <td>4.263003</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.319782</td>\n",
       "      <td>1.283011</td>\n",
       "      <td>0.753812</td>\n",
       "      <td>67.986799</td>\n",
       "      <td>76.744186</td>\n",
       "      <td>81.395349</td>\n",
       "      <td>5.145</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.361191</td>\n",
       "      <td>0.209568</td>\n",
       "      <td>1.925667</td>\n",
       "      <td>69.047619</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>75.925926</td>\n",
       "      <td>6.316001</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.354417</td>\n",
       "      <td>4.059627</td>\n",
       "      <td>0.479963</td>\n",
       "      <td>70.92511</td>\n",
       "      <td>90.625</td>\n",
       "      <td>90.769231</td>\n",
       "      <td>4.054</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.472673</td>\n",
       "      <td>0.278305</td>\n",
       "      <td>0.844189</td>\n",
       "      <td>80.144404</td>\n",
       "      <td>87.5</td>\n",
       "      <td>82.278481</td>\n",
       "      <td>4.992002</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.282652</td>\n",
       "      <td>1.079637</td>\n",
       "      <td>2.456437</td>\n",
       "      <td>84.427481</td>\n",
       "      <td>86.053856</td>\n",
       "      <td>84.491979</td>\n",
       "      <td>10.773001</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.358847</td>\n",
       "      <td>0.177557</td>\n",
       "      <td>1.08269</td>\n",
       "      <td>77.248677</td>\n",
       "      <td>92.592593</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>6.397</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.436385</td>\n",
       "      <td>1.305217</td>\n",
       "      <td>0.872032</td>\n",
       "      <td>74.441687</td>\n",
       "      <td>82.758621</td>\n",
       "      <td>76.521739</td>\n",
       "      <td>7.206997</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.337835</td>\n",
       "      <td>0.196063</td>\n",
       "      <td>0.432705</td>\n",
       "      <td>78.174603</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>93.055556</td>\n",
       "      <td>4.417003</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.47612</td>\n",
       "      <td>0.896915</td>\n",
       "      <td>0.738291</td>\n",
       "      <td>59.927798</td>\n",
       "      <td>85.0</td>\n",
       "      <td>84.810127</td>\n",
       "      <td>4.865001</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.378206</td>\n",
       "      <td>0.489423</td>\n",
       "      <td>0.923583</td>\n",
       "      <td>89.044289</td>\n",
       "      <td>91.803279</td>\n",
       "      <td>88.52459</td>\n",
       "      <td>7.25</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.456581</td>\n",
       "      <td>0.716609</td>\n",
       "      <td>0.606271</td>\n",
       "      <td>80.858086</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>5.206</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.368981</td>\n",
       "      <td>2.428343</td>\n",
       "      <td>0.905215</td>\n",
       "      <td>81.481481</td>\n",
       "      <td>79.62963</td>\n",
       "      <td>92.592593</td>\n",
       "      <td>6.276002</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.324076</td>\n",
       "      <td>1.464786</td>\n",
       "      <td>0.949902</td>\n",
       "      <td>85.478548</td>\n",
       "      <td>90.697674</td>\n",
       "      <td>91.860465</td>\n",
       "      <td>5.277999</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.382546</td>\n",
       "      <td>0.960276</td>\n",
       "      <td>0.549026</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>4.32</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.426608</td>\n",
       "      <td>0.50281</td>\n",
       "      <td>1.248631</td>\n",
       "      <td>62.03966</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.178218</td>\n",
       "      <td>6.168003</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.322073</td>\n",
       "      <td>0.872028</td>\n",
       "      <td>1.358351</td>\n",
       "      <td>83.286119</td>\n",
       "      <td>90.0</td>\n",
       "      <td>84.158416</td>\n",
       "      <td>5.913996</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.25352</td>\n",
       "      <td>0.77125</td>\n",
       "      <td>0.865855</td>\n",
       "      <td>81.949458</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.607595</td>\n",
       "      <td>4.930001</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.36484</td>\n",
       "      <td>0.949123</td>\n",
       "      <td>1.015472</td>\n",
       "      <td>72.456576</td>\n",
       "      <td>86.206897</td>\n",
       "      <td>87.826087</td>\n",
       "      <td>7.023</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.309836</td>\n",
       "      <td>0.328342</td>\n",
       "      <td>1.077296</td>\n",
       "      <td>71.031746</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>84.722222</td>\n",
       "      <td>4.501001</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.325514</td>\n",
       "      <td>3.621152</td>\n",
       "      <td>1.394011</td>\n",
       "      <td>78.2881</td>\n",
       "      <td>71.966912</td>\n",
       "      <td>86.131387</td>\n",
       "      <td>8.28</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.307627</td>\n",
       "      <td>1.518913</td>\n",
       "      <td>2.031804</td>\n",
       "      <td>87.096774</td>\n",
       "      <td>87.931034</td>\n",
       "      <td>86.086957</td>\n",
       "      <td>6.931579</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.58466</td>\n",
       "      <td>0.531269</td>\n",
       "      <td>0.32541</td>\n",
       "      <td>71.365639</td>\n",
       "      <td>78.125</td>\n",
       "      <td>87.692308</td>\n",
       "      <td>4.088</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.38062</td>\n",
       "      <td>0.813759</td>\n",
       "      <td>2.836088</td>\n",
       "      <td>78.253968</td>\n",
       "      <td>90.538194</td>\n",
       "      <td>85.0</td>\n",
       "      <td>10.252001</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.365159</td>\n",
       "      <td>0.561873</td>\n",
       "      <td>0.641713</td>\n",
       "      <td>78.174603</td>\n",
       "      <td>86.111111</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>4.350001</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.313955</td>\n",
       "      <td>4.886589</td>\n",
       "      <td>0.959651</td>\n",
       "      <td>83.862434</td>\n",
       "      <td>85.185185</td>\n",
       "      <td>87.037037</td>\n",
       "      <td>6.289001</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.402634</td>\n",
       "      <td>1.095982</td>\n",
       "      <td>0.764074</td>\n",
       "      <td>69.41896</td>\n",
       "      <td>82.978723</td>\n",
       "      <td>81.914894</td>\n",
       "      <td>5.861004</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.296748</td>\n",
       "      <td>3.418695</td>\n",
       "      <td>2.678212</td>\n",
       "      <td>83.406755</td>\n",
       "      <td>76.941044</td>\n",
       "      <td>82.474227</td>\n",
       "      <td>11.231</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.280963</td>\n",
       "      <td>0.497683</td>\n",
       "      <td>0.492011</td>\n",
       "      <td>82.67148</td>\n",
       "      <td>85.0</td>\n",
       "      <td>92.405063</td>\n",
       "      <td>5.018</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.487386</td>\n",
       "      <td>0.635549</td>\n",
       "      <td>0.908313</td>\n",
       "      <td>61.46789</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>86.170213</td>\n",
       "      <td>5.737</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.27488</td>\n",
       "      <td>0.517639</td>\n",
       "      <td>0.613231</td>\n",
       "      <td>84.70948</td>\n",
       "      <td>85.106383</td>\n",
       "      <td>88.297872</td>\n",
       "      <td>5.665001</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.334293</td>\n",
       "      <td>1.215393</td>\n",
       "      <td>1.141392</td>\n",
       "      <td>83.126551</td>\n",
       "      <td>91.37931</td>\n",
       "      <td>84.347826</td>\n",
       "      <td>6.756</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.302608</td>\n",
       "      <td>0.642884</td>\n",
       "      <td>1.715069</td>\n",
       "      <td>82.750583</td>\n",
       "      <td>88.52459</td>\n",
       "      <td>85.245902</td>\n",
       "      <td>7.137</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.562167</td>\n",
       "      <td>0.506074</td>\n",
       "      <td>0.512553</td>\n",
       "      <td>71.671388</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.108911</td>\n",
       "      <td>6.055001</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.330752</td>\n",
       "      <td>1.327048</td>\n",
       "      <td>2.110223</td>\n",
       "      <td>83.126551</td>\n",
       "      <td>82.758621</td>\n",
       "      <td>80.869565</td>\n",
       "      <td>6.749998</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.415426</td>\n",
       "      <td>0.514612</td>\n",
       "      <td>0.502034</td>\n",
       "      <td>78.174603</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>73.611111</td>\n",
       "      <td>4.251999</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         train_losses valid_losses test_loss train_accs  test_loss   test_acc  \\\n",
       "subjects                                                                        \n",
       "2            0.421393     1.349322    1.5347  85.626911  80.851064  85.106383   \n",
       "3            0.401305     0.297251  0.195776  78.174603  88.888889  93.055556   \n",
       "4            0.370118     0.713013  0.899133  75.577558  90.697674  90.697674   \n",
       "5            0.357124     0.704381  0.892578  84.158416  90.697674   82.55814   \n",
       "6            0.303355     0.951757  1.528855   79.89418  85.185185  86.111111   \n",
       "7             0.37424     1.160006  0.525571  79.761905  88.888889       87.5   \n",
       "8            0.319782     1.283011  0.753812  67.986799  76.744186  81.395349   \n",
       "9            0.361191     0.209568  1.925667  69.047619  88.888889  75.925926   \n",
       "10           0.354417     4.059627  0.479963   70.92511     90.625  90.769231   \n",
       "11           0.472673     0.278305  0.844189  80.144404       87.5  82.278481   \n",
       "12           0.282652     1.079637  2.456437  84.427481  86.053856  84.491979   \n",
       "13           0.358847     0.177557   1.08269  77.248677  92.592593  87.037037   \n",
       "14           0.436385     1.305217  0.872032  74.441687  82.758621  76.521739   \n",
       "15           0.337835     0.196063  0.432705  78.174603  91.666667  93.055556   \n",
       "16            0.47612     0.896915  0.738291  59.927798       85.0  84.810127   \n",
       "17           0.378206     0.489423  0.923583  89.044289  91.803279   88.52459   \n",
       "18           0.456581     0.716609  0.606271  80.858086  86.046512  86.046512   \n",
       "19           0.368981     2.428343  0.905215  81.481481   79.62963  92.592593   \n",
       "20           0.324076     1.464786  0.949902  85.478548  90.697674  91.860465   \n",
       "21           0.382546     0.960276  0.549026  71.428571  83.333333  91.666667   \n",
       "22           0.426608      0.50281  1.248631   62.03966       86.0  82.178218   \n",
       "23           0.322073     0.872028  1.358351  83.286119       90.0  84.158416   \n",
       "24            0.25352      0.77125  0.865855  81.949458       90.0  88.607595   \n",
       "25            0.36484     0.949123  1.015472  72.456576  86.206897  87.826087   \n",
       "26           0.309836     0.328342  1.077296  71.031746  88.888889  84.722222   \n",
       "28           0.325514     3.621152  1.394011    78.2881  71.966912  86.131387   \n",
       "29           0.307627     1.518913  2.031804  87.096774  87.931034  86.086957   \n",
       "30            0.58466     0.531269   0.32541  71.365639     78.125  87.692308   \n",
       "31            0.38062     0.813759  2.836088  78.253968  90.538194       85.0   \n",
       "32           0.365159     0.561873  0.641713  78.174603  86.111111  83.333333   \n",
       "33           0.313955     4.886589  0.959651  83.862434  85.185185  87.037037   \n",
       "34           0.402634     1.095982  0.764074   69.41896  82.978723  81.914894   \n",
       "35           0.296748     3.418695  2.678212  83.406755  76.941044  82.474227   \n",
       "36           0.280963     0.497683  0.492011   82.67148       85.0  92.405063   \n",
       "37           0.487386     0.635549  0.908313   61.46789  85.106383  86.170213   \n",
       "38            0.27488     0.517639  0.613231   84.70948  85.106383  88.297872   \n",
       "39           0.334293     1.215393  1.141392  83.126551   91.37931  84.347826   \n",
       "40           0.302608     0.642884  1.715069  82.750583   88.52459  85.245902   \n",
       "41           0.562167     0.506074  0.512553  71.671388       90.0  89.108911   \n",
       "42           0.330752     1.327048  2.110223  83.126551  82.758621  80.869565   \n",
       "43           0.415426     0.514612  0.502034  78.174603  83.333333  73.611111   \n",
       "\n",
       "         training_time best_epoch  \n",
       "subjects                           \n",
       "2              6.29099         22  \n",
       "3             4.584998         79  \n",
       "4             5.083001         38  \n",
       "5                5.149         50  \n",
       "6             6.293001         52  \n",
       "7             4.263003         53  \n",
       "8                5.145         34  \n",
       "9             6.316001         73  \n",
       "10               4.054         28  \n",
       "11            4.992002         78  \n",
       "12           10.773001         34  \n",
       "13               6.397         91  \n",
       "14            7.206997         52  \n",
       "15            4.417003         86  \n",
       "16            4.865001         29  \n",
       "17                7.25         70  \n",
       "18               5.206         55  \n",
       "19            6.276002         23  \n",
       "20            5.277999         47  \n",
       "21                4.32         50  \n",
       "22            6.168003         88  \n",
       "23            5.913996         32  \n",
       "24            4.930001         29  \n",
       "25               7.023         48  \n",
       "26            4.501001         48  \n",
       "28                8.28         31  \n",
       "29            6.931579         30  \n",
       "30               4.088         99  \n",
       "31           10.252001         33  \n",
       "32            4.350001         57  \n",
       "33            6.289001         16  \n",
       "34            5.861004         65  \n",
       "35              11.231         20  \n",
       "36               5.018         72  \n",
       "37               5.737         41  \n",
       "38            5.665001         66  \n",
       "39               6.756         44  \n",
       "40               7.137         55  \n",
       "41            6.055001         83  \n",
       "42            6.749998         26  \n",
       "43            4.251999         81  "
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = [\"train_losses\", \"valid_losses\", \"test_loss\", \n",
    "    \"train_accs\", \"test_loss\", \"test_acc\", \n",
    "    \"training_time\", \"best_epoch\"]\n",
    "df = pd.DataFrame(columns=col, index=subjects)\n",
    "df.index.name = \"subjects\"\n",
    "\n",
    "for i in range(len(train_losses_list)):\n",
    "    df.loc[subjects[i]] = [train_losses_list[i][-1], valid_losses_list[i][-1], test_loss_list[i], \n",
    "        train_accs_list[i][-1], valid_accs_list[i][-1], test_acc_list[i],\n",
    "        training_time_list[i], best_epoch_list[i]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results\\CM_CW_CNN\\CM_CW_CNN_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_losses      0.370246\n",
       "valid_losses      1.132920\n",
       "test_loss         1.080190\n",
       "train_accs       77.613123\n",
       "test_loss        86.112957\n",
       "test_acc         85.834738\n",
       "training_time     6.032892\n",
       "best_epoch       51.414634\n",
       "dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Best model ====\n",
      "max:  93.05555555555556\n",
      "min:  73.61111111111111\n",
      "mean:  84.37177011009148\n",
      "==== Last (Load) model ====\n",
      "max:  93.05555555555556\n",
      "min:  73.61111111111111\n",
      "mean:  85.8347379564713\n",
      "==== Last model ====\n",
      "max:  93.05555555555556\n",
      "min:  73.61111111111111\n",
      "mean:  85.8347379564713\n"
     ]
    }
   ],
   "source": [
    "def trainedModelLoader(path, device):\n",
    "    model = CM_CW_CNN()\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    model = model.float()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    return model, criterion\n",
    "\n",
    "test_loss_best = []\n",
    "test_acc_best = []\n",
    "test_loss_last = []\n",
    "test_acc_last = []\n",
    "\n",
    "for i, subject in enumerate(subjects):\n",
    "    fname = \"PCA\" if runPCA else \"woPCA\"\n",
    "    if subject<10:\n",
    "        filename = f'{fname}_0{str(subject)}'\n",
    "    else:\n",
    "        filename = f'{fname}_{str(subject)}'\n",
    "    # Best model\n",
    "    type_ = 'Best'\n",
    "    path = f'results/CM_CW_CNN/{type_}/{filename}.pth.tar'\n",
    "    model, criterion = trainedModelLoader(path, device)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator_list[i], criterion)\n",
    "    test_loss_best.append(test_loss)\n",
    "    test_acc_best.append(test_acc)\n",
    "    # Last model\n",
    "    type_ = 'Last'\n",
    "    path = f'results/CM_CW_CNN/{type_}/{filename}.pth.tar'\n",
    "    model, criterion = trainedModelLoader(path, device)\n",
    "    test_loss, test_acc, test_pred_label, test_true_label = evaluate(model, test_iterator_list[i], criterion)\n",
    "    test_loss_last.append(test_loss)\n",
    "    test_acc_last.append(test_acc)\n",
    "\n",
    "\n",
    "print(\"==== Best model ====\")\n",
    "# print(test_acc_best)\n",
    "print(\"max: \", np.max(test_acc_best))\n",
    "print(\"min: \", np.min(test_acc_best))\n",
    "print(\"mean: \", np.mean(test_acc_best))\n",
    "\n",
    "print(\"==== Last (Load) model ====\")\n",
    "# print(test_acc_list)\n",
    "print(\"max: \", np.max(test_acc_last))\n",
    "print(\"min: \", np.min(test_acc_last))\n",
    "print(\"mean: \", np.mean(test_acc_last))\n",
    "\n",
    "print(\"==== Last model ====\")\n",
    "# print(test_acc_list)\n",
    "print(\"max: \", np.max(test_acc_list))\n",
    "print(\"min: \", np.min(test_acc_list))\n",
    "print(\"mean: \", np.mean(test_acc_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cuda.is_available():\n",
    "#     net = model.cuda()\n",
    "#     X_test_tensor = X_test_tensor.cuda()\n",
    "# else:\n",
    "#     net = net.cpu()\n",
    "#     X_test_tensor = X_test_tensor.cpu()\n",
    "\n",
    "# output = net(X_test_tensor)\n",
    "# _, predicted = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(train_losses, label=\"train\")\n",
    "# plt.plot(valid_losses, label=\"validation\")\n",
    "# plt.title(\"Losses\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_accs, label=\"train\")\n",
    "# plt.plot(valid_accs, label=\"validation\")\n",
    "# plt.title(\"Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54d292b6f3ca4ff13f504c55e6e4b729c6c0a14070d37d9d8c8aca786423add6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
